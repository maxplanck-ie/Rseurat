---
title: "First Steps into a Regular Analysis"
date: "`r format(Sys.time(), '%d %B, %Y')`"

output:
  bookdown::html_document2:
    number_sections: false
    global_numbering: true
    theme: spacelab
    highlight: pygments
    code_folding: show
    toc: true
    toc_float: true
    toc_depth: 2
    df_print: paged
---
# Setup
```{r setup}
#.libPath()   # set .libPath 
library(tidyverse)
library(future)
library(Seurat)
library(pheatmap)

set.seed(42)
knitr::opts_chunk$set(echo = TRUE, format = TRUE, out.width = "100%")
options(parallelly.fork.enable = FALSE, future.globals.maxSize = 8*1024^2*1000)
plan("multicore", workers = 8)  # enable parallelization for 8 threads

# useful information
cat("library path: ", .libPaths())
cat("work directory: ", getwd())
```

## Load Data

We will be analyzing a dataset of Peripheral Blood Mononuclear Cells (PBMC) freely available from 10X Genomics. 
There are 2700 single cells that were sequenced with Illumina and aligned to the human transcriptome.

For further details on the primary analysis pipeline that gives you the count data, please head over to [cellranger website](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger).

The raw data can be found [here](https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz), and you have already downloaded it with the repository Zip file. After uncompressin it you wThis includes 3 files: 

- matrix.mtx:  **count matrix** represents the number of molecules for each gene (row) that are detected in each cell (column).
- genes.tsv: a list of ENSEMBL-IDs and their corresponding gene symbol
- barcodes.tsv: a list of molecular barcodes that identifies each cell uniquely

For now we assume that this data resides in a directory `datasets/filtered_gene_bc_matrices/hg19` (relative to this current markdown file). We can read this with a single command:

```{r read10x}
pbmc.data <- Read10X(data.dir = "./datasets/filtered_gene_bc_matrices/hg19/")
```

> âŒ¨ðŸ”¥ Exercises: 

- How many genes and cells does this dataset have?
- Plot the histogram of counts for selected genes
- How many genes are not expressed in any cell (16104, 16634)
- How many cells have less than 1000 counts in total (summed over all genes) 





Lets examine a few genes in the first thirty cells:
```{r explore}
pbmc.data[c("CD3D", "TCL1A", "MS4A1"), 1:30]
```

> âŒ¨ðŸ”¥ Exercises: Draw a pheatmap of the first 1000 genes and 1000 cells

```{r pheatmap_raw}
cols <- colorRampPalette(c("white", "black"))(100)
log10(pbmc.data[1:1000, 1:1000]+1) %>% 
  pheatmap(show_rownames = F, show_colnames = F, cluster_rows = F, cluster_cols = F, color = cols)
```


**Message:** This data is extremely big and sparse !!! We need specialized tools to handle this. Enter Seurat!

Initialize the Seurat object with the raw (non-normalized data):
```{r createSeurat}
pbmc <- CreateSeuratObject(counts = pbmc.data, project = "pbmc3k", min.cells = 3, min.features = 200)
pbmc
```

The `min.cells` and `min.features` arguments are first low-stringency **filters**. We are only loading cells with at least 200 genes detected, and we are only including those genes (features) that were detected in at least 3 cells. 

With these filters in this particular dataset, we are reducing the number of genes from `33000` to `14000`.

The `SeuratObject` serves as a container that contains both data (like the count matrix) and analysis (like PCA, or clustering results) for a single-cell dataset. For example, the count matrix is stored in `pbmc[["RNA"]]@counts`. On RStudio, you can use `View(pbmc)` to inspect all the slots.

> âŒ¨ðŸ”¥ Exercise: In cell "AAATTCGATTCTCA-1", how many reads map to gene "ACTB"? <!-- Hint: subset the sparse matrix in pbmc@assays$RNA -->

At the top level, `SeuratObject` serves as a collection of `Assay` and `DimReduc` objects, representing expression data and dimensional reductions of the expression data, respectively. The `Assay` objects are designed to hold expression data of a single type, such as RNA-seq gene expression, CITE-seq ADTs, cell hashtags, or imputed gene values. On the other hand, `DimReduc` objects represent transformations of the data contained within the Assay object(s) via various dimensional reduction techniques such as PCA. For class-specific details, including more in depth description of the slots, please see the wiki sections for each class:

- [`Seurat`](https://github.com/satijalab/seurat/wiki/Seurat)
  - [Slots](https://github.com/satijalab/seurat/wiki/Seurat#slots)
  - [Object Information](https://github.com/satijalab/seurat/wiki/Seurat#object-information)
  - [Data Access](https://github.com/satijalab/seurat/wiki/Seurat#data-access)
- [`Assay`](https://github.com/satijalab/seurat/wiki/Assay)
  - [Slots](https://github.com/satijalab/seurat/wiki/Assay#slots)
  - [Object Information](https://github.com/satijalab/seurat/wiki/Assay#object-information)
  - [Data Access](https://github.com/satijalab/seurat/wiki/Assay#data-access)
- [`DimReduc`](https://github.com/satijalab/seurat/wiki/DimReduc)
  - [Slots](https://github.com/satijalab/seurat/wiki/DimReduc#slots)
  - [Object Information](https://github.com/satijalab/seurat/wiki/DimReduc#object-information)
  - [Data Access](https://github.com/satijalab/seurat/wiki/DimReduc#data-access)

### Working with Multiple Experiments
[TM: I don't understand the purpose of the goal paragraph. I don't think we need to go deeper into merging.
I use simple `merge(A,B)` on day 4. I think the whole section can go.
Still I have simplified the pseudo-code block below as it was overly complicated]

It's possible to load cells from multiple experiments (e.g. KO, WT) into separate variables, create multiple `SeuratObject` and then merge them. Here's an exemplary data preparation R script. The final instruction saves your file in a binary/ serialized format so that you only need to go through these steps once.

```{r, eval=FALSE}
counts.A <- Read10X('projectA_directory')
counts.B <- Read10X('projectB_directory')

soA <- CreateSeuratObject(counts = counts.A, project = "A")
soA <- CreateSeuratObject(counts = counts.B, project = "B")

myData <- merge( x=A, y=B, merge.data = FALSE, add.cell.ids = c("A", "B") )
saveRDS(myData, file = "output_file.rds")
```

This way, `orig.ident` will be set accordingly. We'll need to come back to this later... for now, let's keep it simple and work with only one level. The pipeline would stay the same at its core, with some extra steps (e.g. batch effects correction, we'll see these on the second half of the course).

# Quality Control

A few QC metrics commonly used, include:

1. **The number of unique genes detected in each cell.**
  - Low-quality cells or empty droplets will often have very few genes.
  - Cell doublets or multiplets may exhibit an aberrant high gene count.
1. Similarly, **the total number of molecules detected within a cell (correlates strongly with unique genes)**
1. **The percentage of reads that map to the mitochondrial genome.**
  - Low-quality / dying cells often exhibit extensive mitochondrial contamination.
  - We use the set of all genes starting with MT- as a set of mitochondrial genes.

For further details, see [this publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4758103/).

The number of unique genes and total molecules are automatically calculated during `CreateSeuratObject()`. You can find them stored in the object `meta.data`, let's see for the first 5 cells:

```{r metadata}
pbmc@meta.data %>% head(5)
```

The `@` operator we just used, is for accessing the slot on the object.

The `[[` operator can add columns to object metadata. This is a great place to stash additional QC stats:

```{r getMT}
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
```

`PercentageFeatureSet()` function calculates the percentage of counts originating from a set of features.
In the example above we can easily access all miochondrial genes because their names start with "^MT".
So we give this as pattern (aka *regular expression*).

Let's visualize the distribution of these metrics over all cells (as Violin plots):

```{r vlnplot}
VlnPlot(pbmc, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```

The `VlnPlot()` function plots the probability density function for all the specified variables (features).

`FeatureScatter()` is typically used to visualize relationships between features, but it can also be used for anything calculated at the object, i.e. columns in object metadata or for genes (rows in the count matrix). 
All those are **features** 

```{r featurescatter}
plot1 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot3 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "ACTB")
plot1 + plot2
plot3
```

# Filtering and Transformation

## Select Cells

Based on cell-specific features we can subset our `SeuratObject` to keep only the 'cells' in good state. In this case, based on the previous Violin plots, we'll use the following criteria:

- Unique feature counts over 2500 or below 200.
- >5% mitochondrial counts.

> âŒ¨ðŸ”¥ Exercise: Which function, also present in base-R and/ or dplyr, would you use: select, filter, or subset?
# I find this reference to dplyr functions more confusing than helful. First; filter ~ subset are very similar and there is
not much of a choice, or? Then I also think of them as row filters. 
Of course in tidyverse rows are samples, and in Seurat columns are samples (cells). 
In any case, all very confusing and I would skip this.


> âŒ¨ðŸ”¥ Exercise: What's the current number of cells after this step? 2583, 2638, 2385?

<!-- DON'T READ THE ANSWER -->

```{r, echo=FALSE}
# notice that the function *subset* has a parameter *subset* tha takes a Boolean vector over samples (cells)
pbmc <- pbmc %>% 
  subset(subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)
```

<!-- DON'T READ THE ANSWER -->

## Normalization

After removing unwanted cells from the dataset, the next step is to normalize the data. By default, we employ a global-scaling normalization method "LogNormalize" that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10000 by default), and log-transforms the result. Normalized values are stored in `pbmc[["RNA"]]@data`.

```{r norm}
pbmc <- NormalizeData(pbmc)
```

## Gene Selection

The main goal is to select (biologically informative) genes that will help us to organize cells according to the
transcription profile.
Therefore we look for a subset of genes ("features") that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). 
To complicate matters, genes with different expression level tend to have different variance, so we need to account for this.

To identify highly variable genes, Seurat models the mean-variance relationship inherent in the data using the `FindVariableFeatures()` function.

By default, it uses the `vst` methodology with 2000 features per dataset. First, fits a line to the relationship of `log(variance)` and `log(mean)` using local polynomial regression (`loess`). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (by default, square root of the number of cells). These will be used downstream in dimensional reductions like PCA. 

<!--Some relevant publications for Feature selection and its importance are: [[1](https://www.nature.com/articles/nmeth.2645)], and [[2](https://doi.org/10.1016/j.cell.2019.05.031)].-->

```{r}
pbmc <- FindVariableFeatures(pbmc)
```

<!--Note that there's also the `selection.method = "mvp"` that identifies variable features while controlling for the strong relationship between variability and average expression.-->

> âŒ¨ðŸ”¥ Exercise: Which are the 10 most highly variable genes? Hint: use `VariableFeatures()`.

```{r, echo=FALSE}
## DON'T READ THE ANSWER
# (top10 <- VariableFeatures(pbmc) %>% head(10))
```

> âŒ¨ðŸ”¥ Exercise: What's the variance of the gene `PYCARD`? hint: use `HVFInfo()` over your `assay`.

```{r, echo=FALSE}
## DON'T READ THE ANSWER
HVFInfo(pbmc@assays$RNA, selection.method = "vst")["PYCARD",]$variance.standardized
HVFInfo(pbmc) %>% filter( rownames(.) == "PYCARD")
```


Plot variable features

```{r}
(plot1 <- VariableFeaturePlot(pbmc))
```

Now with labels, taking top10 genes as in the recent exercise:

```{r}
(plot2 <-
   LabelPoints(
     plot = plot1,
     points = head(VariableFeatures(pbmc), 10),
     repel = TRUE
   ))
```

## Scaling

Next, we apply a linear transformation ('scaling') that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The `ScaleData()` function:

- Shifts the expression of each gene, so that the mean expression across cells is `0`
- Scales the expression of each gene, so that the variance across cells is `1`. This step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate.
- The results of this are stored in `pbmc[["RNA"]]@scale.data`


```{r}
pbmc <- ScaleData(pbmc, features = rownames(pbmc))
```

# Dimensional Reduction

Next we perform PCA on the scaled data. By default, only the previously determined variable features are used as input, but can be defined using features argument if you wish to choose a different subset.

```{r}
pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
```

> Do you feel like you need a refresher on PCA? check [StatQuest with Josh Starmer video](https://youtu.be/FgakZw6K1QQ) explaining PCA by SVD step by step! (duration: 20 minutes)

Examine and visualize PCA results a few different ways:

```{r}
print(pbmc[["pca"]], dims = 1:5, nfeatures = 5)
```

```{r}
VizDimLoadings(pbmc, dims = 1:2, reduction = "pca")
```

```{r}
DimPlot(pbmc, reduction = "pca") + NoLegend()
```

In particular `DimHeatmap()` allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting cells to a number plots the 'extreme' cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, we find this to be a valuable tool for exploring correlated feature sets.

```{r}
DimHeatmap(pbmc, dims = 1:9, cells = 500, balanced = TRUE)
```

To overcome the extensive technical noise in any single gene for scRNA-seq data, Seurat clusters cells based on their PCA scores. Here each PC essentially represents a 'metagene' that combines information across a correlated gene sets. The top principal components therefore represent a robust compression of the dataset.

One quick way to determine the 'dimensionality' of the dataset is by eyeballing how the percentage of variance explained decreases:

```{r}
ElbowPlot(pbmc)
```

> âŒ¨ðŸ”¥ Poll: How many components should we choose to include? 10? 20? 100?


[TM: removed Jackstraw procedure. Perhaps one can add a citation instead?]


```{r, eval=FALSE}
# The JackStraw approach permutes the data and repeats the PCA maany times
# This is to rationalize the search for the "elbow" with some significance testing
# It takes long, so we will not execute it
pbmc <- JackStraw(pbmc, num.replicate = 100)
pbmc <- ScoreJackStraw(pbmc, dims = 1:15)
JackStrawPlot(pbmc, dims = 1:15)
```


Anything between 7-12 would be acceptable, and **it's better to err on the higher side**!


[TM: removed exercise to adjust markdown parameters ] 

# BREAK

# Clustering

[TM: I'm not sure if this is the best reference. Graph-based clustring is much older than PhenoGraph. Perhaps MCL would be better?]
The next step follow the work pioneered by `PhenoGraph`, a robust computational method that partitions high-dimensional single-cell data into subpopulations. Building on these subpopulations, `PhenoGraph` authors developed additional methods to extract high-dimensional signaling phenotypes and infer differences in functional potential between subpopulations. For details, be sure to check the [research paper](http://www.ncbi.nlm.nih.gov/pubmed/26095251).

This subpopulations _could_ be of biological relevance, retrieving these is our goal. The definition of such groupings depend upon the parameters. This algorithm in particular is the K Nearest Neighbors (KNN) graph that is constructed based on the euclidean distance in PCA space. For an example of building such a graph, imagine we took only two PCs (principal components) and had such an arrangement of cells like these dots in a 2D plane...

```{r, echo=FALSE, fig.cap="Example of k-NN classification for a cell highlighted in green color."}
knitr::include_graphics("./images/KnnClassification.svg")
```

The test sample (green dot) should be classified either to the group made of blue squares or to the subpopulation of cells here represented in red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).

There's a drawback with the 'majority voting' scheme, the assignment of such clusters is biased towards the clusters that have greater number of members (especially when ties start appearing). For that reason, we'll refine the process by using a graph, where edge weights between any two cells is based on the shared overlap in their local neighborhoods (Jaccard similarity).




$$
J(A,B) = \frac{Intersect(A,B)}{Union(A,B)} = J(B,A)
$$
[TM: Perhaps one can illustrate formula with a simple Venn Diagram]

**If two datasets share the exact same members, their Jaccard Similarity Index will be 1. Conversely, if they have no members in common then their similarity will be 0.**

All the process described before, including the use of Jaccard Similarity Index is performed using the `FindNeighbors()` function, and takes as input the previously defined dimensionality of the dataset (first 10 PCs). So the KNN is build using multidimensional space, but the rules we just saw for 2D still apply.

> âŒ¨ðŸ”¥ Poll:  Which is the default value that Seurat uses? Hint: `help` yourselves by reading the docs!

```{r findneighbours}
Ndims=10  # Choice baased on ElbowPlot (or JackStraw)
pbmc <- FindNeighbors(pbmc, dims = 1:Ndims)
```

We want to keep our clusters looking natural. That is, we want to have a modularity optimization on top of all. For that, we'll use the community search algorithm for graphs called Louvain. You can read more about it [here](https://scribe.froth.zone/louvain-algorithm-93fde589f58c?gi=8dce35ee2f75), or to read about it being applied to the biological problem at hand look into [this research paper](http://dx.doi.org/10.1088/1742-5468/2008/10/P10008).

The `FindClusters()` function implements this procedure, and contains a `resolution` parameter that sets the 'granularity' of the downstream clustering, with increased values leading to a greater number of clusters. Usually, setting this parameter between `0.4` and `1.2` returns good results for single-cell datasets of around 3K cells. You can easily try various values and see how it performs. Optimal resolution often increases for larger datasets.

```{r findclusters}
pbmc <- FindClusters(pbmc, resolution = 0.5)
```

[TM: The Idents() function falls from the sky. I think they should first look at pbmc@metadata %>% head() to find all possible identities]

The clusters can be found using the `Idents()` function. Here, for example we can look at cluster IDs of the first 5 cells:

```{r idents}
Idents(pbmc) %>% head(5)
```

> âŒ¨ðŸ”¥ Exercise: How many clusters did we find?

NOTE: The [upcoming v5](https://doi.org/10.1101/2022.02.24.481684) of Seurat will include a new clustering method [(specifically designed for handling data sets with the number of cells in the order of millions.)](https://satijalab.org/seurat/articles/seurat5_sketch_analysis.html)


Save your work
```{r save}
saveRDS(pbmc, file="my_pbmc.rds")
sessioninfo::session_info(to_file="my_session")
```
# End

[TM: removed all homework. This was overly focussed on rmd issues which we don't teach. ]