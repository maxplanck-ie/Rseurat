---
title: "First Steps into a Regular Analysis"
date: "`r format(Sys.time(), '%d %B, %Y')`"

output:
  bookdown::html_document2:
    theme: spacelab
    highlight: monochrome
    code_folding: show
    toc: true
    toc_float: true
    toc_depth: 2
    df_print: paged

params:
  seed:
    label: 'Random seed:'
    value: 8211673
  Nthreads:
    label: 'Number of Threads:'
    value: 8
    input: slider
    min: 4
    max: 96
  Nmemgb:
    label: 'Gigabytes of Memory per Thread:'
    value: 8
    input: slider
    min: 4
    max: 16
  fork:
    label: 'Fork processes (not GUI)'
    value: FALSE
    input: checkbox
---

<!-- https://bookdown.org/yihui/rmarkdown-cookbook/parameterized-reports.html -->

# Setup

```{r}
timestamp()
```

```{r}
getwd()
```

```{r}
cbind(params)
```

```{r}
my_load_namespaces <- function(packagesToLoad=c(), coreLibraries=c()) {
  # coreLibraries = not explicitly loaded, but required anyway (e.g. dependencies)
  installed_pkgs <- installed.packages()
  
  if (! "BiocManager" %in% installed_pkgs) install.packages("BiocManager")
  
  lapply(c(coreLibraries, packagesToLoad),
         function(x) if (! x %in% installed_pkgs) BiocManager::install(x, ask = FALSE))
  
  lapply(packagesToLoad, function(pkg) {
    suppressPackageStartupMessages(require(pkg, character.only = TRUE))
  })
  
  TRUE
}
```

```{r}
my_load_namespaces(
  packagesToLoad = c("openxlsx", "readr", "magrittr", "dplyr", "RColorBrewer", "ggplot2", "patchwork", "Seurat", "future"),
  coreLibraries = c("knitr", "rmarkdown", "formatR", "DT", "reticulate", "sessioninfo", "limma", "DESeq2", "uwot", "future.apply", "metap", "enrichR", "shiny", "SingleCellExperiment", "remotes", "bookdown")
)
```

```{r}
if (interactive()) stopifnot(suppressMessages(BiocManager::valid()))

set.seed(params$seed)
knitr::opts_chunk$set(echo = TRUE, format=TRUE)

options(parallelly.fork.enable = params$fork,
        future.globals.maxSize = params$Nmemgb * 1024^2 * 1000)

plan("multicore", workers = params$Nthreads)

myFlappy <- function(x, ...) {
  future.apply::future_lapply(x, ..., future.seed = TRUE)
}
```

## Load Data

```{r}
dataSourceURL <- "https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz"
```

We will be analyzing a dataset of Peripheral Blood Mononuclear Cells (PBMC) freely available from 10X Genomics. There are 2,700 single cells that were sequenced on the Illumina NextSeq 500. The raw data can be found here: `r dataSourceURL`.

For further details on the primary analysis pipeline, see [here](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger).

The values in the **count matrix** represent the number of molecules for each feature (i.e. gene; row) that are detected in each cell (column).

```{r}
pbmc.data <- Read10X(data.dir = "./datasets/filtered_gene_bc_matrices/hg19/")
```

Lets examine a few genes in the first thirty cells:

```{r}
pbmc.data[c("CD3D", "TCL1A", "MS4A1"), 1:30]
```

Initialize the Seurat object with the raw (non-normalized data):

```{r}
pbmc <- CreateSeuratObject(counts = pbmc.data, project = "pbmc3k", min.cells = 3, min.features = 200)
pbmc
```

The `SeuratObject` serves as a container that contains both data (like the count matrix) and analysis (like PCA, or clustering results) for a single-cell dataset. For example, the count matrix is stored in `pbmc[["RNA"]]@counts`. On RStudio, you can use `View(pbmc)` to inspect all the slots.

At the top level, `SeuratObject` serves as a collection of `Assay` and `DimReduc` objects, representing expression data and dimensional reductions of the expression data, respectively. The `Assay` objects are designed to hold expression data of a single type, such as RNA-seq gene expression, CITE-seq ADTs, cell hashtags, or imputed gene values. On the other hand, `DimReduc` objects represent transformations of the data contained within the Assay object(s) via various dimensional reduction techniques such as PCA. For class-specific details, including more in depth description of the slots, please see the wiki sections for each class:

- [`Seurat`](https://github.com/satijalab/seurat/wiki/Seurat)
  - [Slots](https://github.com/satijalab/seurat/wiki/Seurat#slots)
  - [Object Information](https://github.com/satijalab/seurat/wiki/Seurat#object-information)
  - [Data Access](https://github.com/satijalab/seurat/wiki/Seurat#data-access)
- [`Assay`](https://github.com/satijalab/seurat/wiki/Assay)
  - [Slots](https://github.com/satijalab/seurat/wiki/Assay#slots)
  - [Object Information](https://github.com/satijalab/seurat/wiki/Assay#object-information)
  - [Data Access](https://github.com/satijalab/seurat/wiki/Assay#data-access)
- [`DimReduc`](https://github.com/satijalab/seurat/wiki/DimReduc)
  - [Slots](https://github.com/satijalab/seurat/wiki/DimReduc#slots)
  - [Object Information](https://github.com/satijalab/seurat/wiki/DimReduc#object-information)
  - [Data Access](https://github.com/satijalab/seurat/wiki/DimReduc#data-access)

### NOTE

It's possible to load cells from multiple experiments (e.g. KO, WT) into separate variables, create multiple `SeuratObject` and then merge them. Here's an exemplary data preparation R script. The final instruction saves your file in a binary/ serialized format so that you only need to go through these steps once.

```{r, eval=FALSE}
root.dir <- file.path("/data/something/here")

counts.MyWT <- Read10X(
  file.path(
    root.dir, "MyWT", "outs/filtered_feature_bc_matrix"
    )
  )

counts.MyKO <- Read10X(
  file.path(
    root.dir, "MyKO", "outs/filtered_feature_bc_matrix"
    )
  )

wt <- CreateSeuratObject(
  counts = counts.MyWT,
  project = "WT"
  )

ko <- CreateSeuratObject(
  counts = counts.MyKO,
  project = "KO"
  )

myData <- merge(
  x=wt, y=ko,
  merge.data = FALSE,
  add.cell.ids = c("WT", "KO")
  )
# Note about merge():
# This functions extends base R to work with SeuratObj,
# you can see this by consulting its help page.
#
# If you had three or more datasets to merge,
# turn the second argument in a lengthier vector.
# And extend add.cell.ids proportionally.

saveRDS(myData, file = file.path(getwd(), "resources/raw_data/myData.rds"))
```

This way, `orig.ident` will be set accordingly. We'll need to come back to this later... for now, let's keep it simple and work with only one level. The pipeline would stay the same at its core, with some extra steps (e.g. batch effects correction, we'll see these on the second half of the course).

# Quality Control

A few QC metrics commonly used, include:

1. **The number of unique genes detected in each cell.**
  - Low-quality cells or empty droplets will often have very few genes.
  - Cell doublets or multiplets may exhibit an aberrant high gene count.
1. Similarly, **the total number of molecules detected within a cell (correlates strongly with unique genes)**
1. **The percentage of reads that map to the mitochondrial genome.**
  - Low-quality / dying cells often exhibit extensive mitochondrial contamination.
  - We use the set of all genes starting with MT- as a set of mitochondrial genes.

For further details, see [this publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4758103/).

The number of unique genes and total molecules are automatically calculated during `CreateSeuratObject()`. You can find them stored in the object `meta.data`, let's see for the first 5 cells:

```{r}
head(pbmc@meta.data, 5)
```

The `@` operator we just used, is for accessing the slot on the object.

The `[[` operator can add columns to object metadata. This is a great place to stash QC stats:

```{r}
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
```

`PercentageFeatureSet()` function calculates the percentage of counts originating from a set of features.

Let's visualize these broad metrics now as Violin plots.:

```{r}
VlnPlot(pbmc, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```

The `VlnPlot()` function allows to see the probability density function, along with the actual values, for any variable.

`FeatureScatter()` is typically used to visualize feature-feature relationships, but can be used for anything calculated at the object, i.e. columns in object metadata, PC scores etc.

```{r}
plot1 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2
```

<!-- ## Cell-Cycle scoring

This is a bit of a de-tour. To keep the analysis simple and up to the most standard way, we won't be trying to mitigate the effects of cell cycle heterogeneity in the dataset today. Follow the [tutorial](https://satijalab.org/seurat/articles/cell_cycle_vignette.html) if the need arises when working with your own dataset.

We assign scores in the `CellCycleScoring()` function, which stores S and G2/M scores in object meta data, along with the predicted classification of each cell in either G2M, S or G1 phase.

``{r}
standardize.gene.name <- function(gene_name, sp) {
  if (sp == "mouse") {
    x <- tools::toTitleCase(tolower(gene_name))
  } else if (sp == "human") {
    x <- toupper(gene_name)
  }
  x
} 

pbmc %<>% CellCycleScoring(s.features = standardize.gene.name(cc.genes$s.genes, sp="human"),
                           g2m.features = standardize.gene.name(cc.genes$g2m.genes, sp="human"),
                           nbin = 20)

# To allow using 'alternate' cell-cycle regress-out @ PCA
pbmc$CC.Difference <- pbmc$S.Score - pbmc$G2M.Score
`` -->


## Filter

We'll filter cells that have:

- Unique feature counts over 2500 or below 200.
- >5% mitochondrial counts.

```{r}
pbmc <- subset(pbmc, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)
```

Note that the `subset()` function we just used is not the same than in base R!

We started with 2700 cells. POLL: What's the current number of cells? 2583, 2638, 2385?

# Normalization

After removing unwanted cells from the dataset, the next step is to normalize the data. By default, we employ a global-scaling normalization method "LogNormalize" that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10000 by default), and log-transforms the result. Normalized values are stored in `pbmc[["RNA"]]@data`.

```{r}
pbmc <- NormalizeData(pbmc)
```

## Feature Selection

Next, we look at a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). To identify highly variable features, Seurat models the mean-variance relationship inherent in the data using the `FindVariableFeatures()` function.

By default, it uses the "vst" methodology with 2000 features per dataset. First, fits a line to the relationship of log(variance) and log(mean) using local polynomial regression (loess). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (by default, square root of the number of cells). These will be used downstream in dimensional reductions like PCA. 

<!--Some relevant publications for Feature selection and its importance are: [[1](https://www.nature.com/articles/nmeth.2645)], and [[2](https://doi.org/10.1016/j.cell.2019.05.031)].-->

```{r}
pbmc <- FindVariableFeatures(pbmc)
```

<!--Note that there's also the `selection.method = "mvp"` that identifies variable features while controlling for the strong relationship between variability and average expression.-->

Identify the 10 most highly variable genes:

```{r}
(top10 <- head(VariableFeatures(pbmc), 10))
```

Plot variable features

```{r}
(plot1 <- VariableFeaturePlot(pbmc))
```

Now with labels:

```{r}
(plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE))
```

## Scaling

Next, we apply a linear transformation ('scaling') that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The `ScaleData()` function:

- Shifts the expression of each gene, so that the mean expression across cells is `0`
- Scales the expression of each gene, so that the variance across cells is `1`. This step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate.
- The results of this are stored in `pbmc[["RNA"]]@scale.data`


```{r}
pbmc <- ScaleData(pbmc, features = rownames(pbmc))
```

# Dimensional Reduction

Next we perform PCA on the scaled data. By default, only the previously determined variable features are used as input, but can be defined using features argument if you wish to choose a different subset.

```{r}
pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
```

> Do you feel like you need a refresher on PCA? check [StatQuest with Josh Starmer video](https://youtu.be/FgakZw6K1QQ) explaining PCA by SVD step by step! (duration: 20 minutes)

Examine and visualize PCA results a few different ways:

```{r}
print(pbmc[["pca"]], dims = 1:5, nfeatures = 5)
```

```{r}
VizDimLoadings(pbmc, dims = 1:2, reduction = "pca")
```

```{r}
DimPlot(pbmc, reduction = "pca") + NoLegend()
```

In particular `DimHeatmap()` allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting cells to a number plots the 'extreme' cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, we find this to be a valuable tool for exploring correlated feature sets.

```{r}
DimHeatmap(pbmc, dims = 1:9, cells = 500, balanced = TRUE)
```

To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a 'metafeature' that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset.

POLL: How many components should we choose to include? 10? 20? 100?

One quick way to determine the 'dimensionality' of the dataset is by eyeballing how the percentage of variance explained decreases:

```{r}
ElbowPlot(pbmc)
```

Another one, is using the JackStraw procedure: We randomly permute a subset of the data (1% by default) and rerun PCA, constructing a 'null distribution' of feature scores, and repeat this procedure. We identify 'significant' PCs as those who have a strong enrichment of low p-value features.

> Do you feel like you need a refresher on permutations? check this beautiful animation of permutations explained in the context of statistical testing (constructing a 'null distribution'): <https://www.jwilber.me/permutationtest/>

```{r}
# NOTE: This process can take a long time for big datasets,
# There's nothing wrong with using ElbowPlot()!
pbmc <- JackStraw(pbmc, num.replicate = 100)
```

```{r}
pbmc <- ScoreJackStraw(pbmc, dims = 1:15)
```

```{r}
JackStrawPlot(pbmc, dims = 1:15)
```

Anything between 7-12 would be acceptable, and **it's better to err on the higher side**!

```{r}
Ndim=10
```

> EXERCISE: comment the `Ndim` code block, and convert this into a parameter of the current Rmarkdown document.

# Clustering

The next step follow the work pioneered by `PhenoGraph`, a robust computational method that partitions high-dimensional single-cell data into subpopulations. Building on these subpopulations, `PhenoGraph` authors developed additional methods to extract high-dimensional signaling phenotypes and infer differences in functional potential between subpopulations. For details, be sure to check the [research paper](http://www.ncbi.nlm.nih.gov/pubmed/26095251).

This subpopulations _could_ be of biological relevance, retrieving these is our goal. The definition of such groupings depend upon the parameters we use in our algorithms (Of course, this is not a strong dependency). This algorithm in particular is the K Nearest Neighbors (KNN) graph that is constructed based on the euclidean distance in PCA space. For an example of building such a graph, imagine we took only two PCs (principal components) and had such an arrangement of cells like these dots in a 2D plane...

```{r, echo=FALSE, fig.cap="Example of k-NN classification for a cell highlighted in green color."}
knitr::include_graphics("./images/KnnClassification.svg")
```

The test sample (green dot) should be classified either to the group made of blue squares or to the subpopulation of cells here represented in red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).

There's a drawback with the 'majority voting' scheme, the assignment of such clusters is biased towards the clusters that have greater number of members (especially when ties start appearing). For that reason, we'll refine the edge weights between any two cells based on the shared overlap in their local neighborhoods (Jaccard similarity). If two datasets share the exact same members, their Jaccard Similarity Index will be 1. Conversely, if they have no members in common then their similarity will be 0.

```{r, echo=FALSE, fig.cap="Jaccard Index is calculated using the Union and the Intersection between sets."}
knitr::include_graphics("./images/jaccard.svg")
```

All the process described before is performed using the `FindNeighbors()` function, and takes as input the previously defined dimensionality of the dataset (first 10 PCs). So the KNN is build using multidimensional space, but the rules we just saw for 2D still apply.

POLL: Machine learning algorithms have parameters and hyperparameters, Deep Learning Large Language Models have them in the order of billions, this one only has one: `K`. Can you find which is the default value that Seurat uses? 10? 20? 30?... Hint: `help` yourselves by reading the docs!

```{r}
pbmc <- FindNeighbors(pbmc, dims = 1:10)
```

We want to keep our clusters looking natural. That is, we want to have a modularity optimization on top of all. For that, we'll use the community search algorithm for graphs called Louvain. You can read more about it [here](https://scribe.froth.zone/louvain-algorithm-93fde589f58c?gi=8dce35ee2f75), or to read about it being applied to the biological problem at hand look into [this research paper](http://dx.doi.org/10.1088/1742-5468/2008/10/P10008).

The `FindClusters()` function implements this procedure, and contains a `resolution` parameter that sets the 'granularity' of the downstream clustering, with increased values leading to a greater number of clusters. Usually, setting this parameter between `0.4` and `1.2` returns good results for single-cell datasets of around 3K cells. You can easily try various values and see how it performs. Optimal resolution often increases for larger datasets.

```{r}
pbmc <- FindClusters(pbmc, resolution = 0.5)
```

The clusters can be found using the `Idents()` function. Here, for example we can look at cluster IDs of the first 5 cells:

```{r}
head(Idents(pbmc), 5)
```

POLL: How many clusters did we find?

NOTE: The [upcoming v5](https://doi.org/10.1101/2022.02.24.481684) of Seurat will include a new clustering method [(specifically designed for handling data sets with the number of cells in the order of millions.)](https://satijalab.org/seurat/articles/seurat5_sketch_analysis.html)

# EXERCISES

> These are **not mandatory**, and barely related to scRNAseq... we hope it will add to your 'Working with RMarkdown' skills.

1. Save all your R session details (a.k.a. environment specification) using the function `sessioninfo::session_info()` and its `to_file` argument.

2. Did you just use a string to hardcode the path? How about adding the date inside the filename? E.g. `./RseuratCourseRootDir/config/Session_Tue_Mar__7_13-58-08_2023.log`... it would be cumbersome and error-prone to keep updating this each time!


> Hint: `gsub(' ','_',gsub(':','-', date())` will get you the date in the format we just showed, using dashes and underscores for safety (POSIX principles?). Also, be sure to use `file.path()`, `getwd()`, `paste0()`, etc. to make it look fancy, and get around all the awkwardness and rough edges... e.g. Does `file.path` add or lack a trailing `/`? If we use `getwd` but this is run inside the notebook subfolder `./rmd/`, don't we need to append a `..`? Check the output of `getwd()` interactively and on the rendered markdown document (in fact, we added this command as the 2nd code block in this document just for this very reason :P)

3. Implement the definition of the remaining parameters (dimensionality of the dataset, and cluster resolution) in the YAML params R list object.

4. Implement an overwrite of the knit function so that you save the HTML report somewhere else in the project folder, like a `lectures/` sub folder.

> Hint: here's the YAML element to add in your `Rmd` header:
> 
> ```{r, echo=TRUE, eval=FALSE}
> knit: (function(inputFile, encoding) {
>   rmarkdown::render(inputFile, encoding = encoding,
>   output_file = file.path(getwd(),
>   '../lectures', paste0(gsub(' ','_',gsub(':','-',
>   date())), '_results.html')))})
> ```
> 
> NOTE: trailing slash in 'lectures' subfolder is implicit by the usage of multiple arguments within the call to `file.path()`...

5. Implement a save function that writes your single `SeuratObject` to an RDS file, and name it with all the parameters in use.

6. Confirm that you used the correct function (we asked for RDS file!), by reading the data into a new R session using the function `readRDS`.

7. Re-run this analysis with another set of parameters. See that files are not overwritten, and that you can track which RDS and HTML Report corresponds to; in each case.
